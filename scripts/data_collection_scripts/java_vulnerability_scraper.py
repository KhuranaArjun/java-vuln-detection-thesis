#!/usr/bin/env python3
"""
Java Vulnerability Dataset Generator - Step 1: GitHub Commit Scraper
Based on Laura Wartschinski's VUDENC methodology, adapted for Java

This script scrapes vulnerability-fixing commits from GitHub repositories
containing Java code using security-related keywords.
"""

import requests
import json
import time
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JavaVulnerabilityCommitScraper:
    def __init__(self, github_token: str):
        """
        Initialize the scraper with GitHub API token
        
        Args:
            github_token: GitHub personal access token
        """
        self.github_token = github_token
        self.base_url = "https://api.github.com"
        self.headers = {
            "Authorization": f"token {github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        self.rate_limit_remaining = 5000
        self.rate_limit_reset = None
        
        # Java-specific vulnerability keywords (based on Wartschinski's approach)
        self.vulnerability_keywords = [
            # General security terms
            "security fix", "vulnerability", "CVE", "security patch", "security issue",
            "exploit", "malicious", "injection", "XSS", "CSRF", "authentication bypass",
            
            # Java-specific vulnerabilities
            "deserialization", "XXE", "XML external entity", "path traversal",
            "SQL injection", "LDAP injection", "command injection", "code injection",
            "unsafe reflection", "unsafe deserialization", "serialization vulnerability",
            
            # Java framework specific
            "Spring security", "Struts vulnerability", "JSF security", "Hibernate injection",
            "Jackson deserialization", "JNDI injection", "log4j vulnerability",
            
            # Common patterns
            "sanitize", "validate input", "escape", "prevent injection",
            "security validation", "input validation", "output encoding"
        ]
        
        # Repository quality filters (following Wartschinski's filtering approach)
        self.excluded_keywords = [
            "ctf", "capture-the-flag", "vulnerability-demo", "exploit-demo",
            "hacking-tutorial", "security-demo", "poc", "proof-of-concept",
            "vulnerable-app", "intentionally-vulnerable", "dvwa", "webgoat"
        ]
    
    def check_rate_limit(self):
        """Check and handle GitHub API rate limits"""
        if self.rate_limit_remaining <= 10:
            if self.rate_limit_reset:
                sleep_time = max(0, self.rate_limit_reset - time.time() + 60)
                logger.info(f"Rate limit reached. Sleeping for {sleep_time:.0f} seconds...")
                time.sleep(sleep_time)
    
    def make_api_request(self, url: str, params: Dict = None) -> Optional[Dict]:
        """Make API request with rate limiting and error handling"""
        self.check_rate_limit()
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            
            # Update rate limit info
            self.rate_limit_remaining = int(response.headers.get('X-RateLimit-Remaining', 0))
            if 'X-RateLimit-Reset' in response.headers:
                self.rate_limit_reset = int(response.headers['X-RateLimit-Reset'])
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 403:
                logger.warning("Rate limit exceeded, waiting...")
                time.sleep(3600)  # Wait 1 hour
                return self.make_api_request(url, params)
            else:
                logger.error(f"API request failed: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"Request error: {e}")
            return None
    
    def search_repositories(self, max_repos: int = 1000) -> List[Dict]:
        """
        Search for Java repositories that might contain vulnerability fixes
        
        Args:
            max_repos: Maximum number of repositories to collect
            
        Returns:
            List of repository information
        """
        repositories = []
        
        # Search queries for Java repositories with security content
        search_queries = [
            "language:java security vulnerability stars:>10",
            "language:java CVE fix stars:>5",
            "language:java security patch stars:>5",
            "language:java SQL injection fix",
            "language:java XSS fix",
            "language:java deserialization fix",
            "language:java Spring security fix"
        ]
        
        for query in search_queries:
            if len(repositories) >= max_repos:
                break
                
            page = 1
            while len(repositories) < max_repos:
                logger.info(f"Searching repositories with query: '{query}' (page {page})")
                
                url = f"{self.base_url}/search/repositories"
                params = {
                    'q': query,
                    'sort': 'stars',
                    'order': 'desc',
                    'per_page': 100,
                    'page': page
                }
                
                data = self.make_api_request(url, params)
                if not data or 'items' not in data:
                    break
                
                repos = data['items']
                if not repos:
                    break
                
                for repo in repos:
                    if self.is_valid_repository(repo):
                        repositories.append({
                            'name': repo['full_name'],
                            'id': repo['id'],
                            'stars': repo['stargazers_count'],
                            'size': repo['size'],
                            'language': repo['language'],
                            'default_branch': repo['default_branch'],
                            'clone_url': repo['clone_url']
                        })
                
                page += 1
                if page > 10:  # Limit pages per query
                    break
                    
                time.sleep(1)  # Be nice to the API
        
        # Remove duplicates
        unique_repos = []
        seen_names = set()
        for repo in repositories:
            if repo['name'] not in seen_names:
                unique_repos.append(repo)
                seen_names.add(repo['name'])
        
        logger.info(f"Found {len(unique_repos)} unique repositories")
        return unique_repos[:max_repos]
    
    def is_valid_repository(self, repo: Dict) -> bool:
        """
        Check if repository is valid for vulnerability dataset creation
        Following Wartschinski's filtering approach
        """
        repo_name = repo['full_name'].lower()
        description = (repo.get('description') or '').lower()
        
        # Check for excluded keywords
        for keyword in self.excluded_keywords:
            if keyword in repo_name or keyword in description:
                return False
        
        # Ensure it's a Java repository with reasonable activity
        if (repo.get('language') != 'Java' or 
            repo.get('stargazers_count', 0) < 5 or
            repo.get('size', 0) < 100):  # Filter very small repos
            return False
        
        return True
    
    def search_vulnerability_commits(self, repo_name: str, max_commits: int = 100) -> List[Dict]:
        """
        Search for vulnerability-fixing commits in a specific repository
        
        Args:
            repo_name: Repository full name (owner/repo)
            max_commits: Maximum commits to collect per repository
            
        Returns:
            List of commit information
        """
        commits = []
        
        # Search in commit messages for vulnerability-related terms
        for keyword in self.vulnerability_keywords:
            if len(commits) >= max_commits:
                break
                
            try:
                url = f"{self.base_url}/search/commits"
                params = {
                    'q': f'repo:{repo_name} "{keyword}"',
                    'sort': 'committer-date',
                    'order': 'desc',
                    'per_page': 30
                }
                
                # Note: Commit search requires special accept header
                headers = self.headers.copy()
                headers['Accept'] = 'application/vnd.github.cloak-preview'
                
                response = requests.get(url, headers=headers, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    for commit in data.get('items', []):
                        if len(commits) >= max_commits:
                            break
                            
                        if self.is_valid_commit(commit, keyword):
                            commits.append({
                                'sha': commit['sha'],
                                'message': commit['commit']['message'],
                                'author': commit['commit']['author']['name'],
                                'date': commit['commit']['author']['date'],
                                'url': commit['html_url'],
                                'keyword_matched': keyword,
                                'repository': repo_name
                            })
                
                time.sleep(2)  # Rate limiting for commit search
                
            except Exception as e:
                logger.error(f"Error searching commits for keyword '{keyword}': {e}")
                continue
        
        logger.info(f"Found {len(commits)} vulnerability commits in {repo_name}")
        return commits
    
    def is_valid_commit(self, commit: Dict, keyword: str) -> bool:
        """
        Validate if commit is likely a vulnerability fix
        """
        message = commit['commit']['message'].lower()
        
        # Must contain the keyword
        if keyword.lower() not in message:
            return False
        
        # Should contain fix-related terms
        fix_terms = ['fix', 'patch', 'resolve', 'prevent', 'secure', 'sanitize']
        if not any(term in message for term in fix_terms):
            return False
        
        # Exclude merge commits and other non-fixes
        exclude_terms = ['merge', 'revert', 'format', 'style', 'comment', 'doc']
        if any(term in message for term in exclude_terms):
            return False
        
        return True
    
    def get_commit_diff(self, repo_name: str, commit_sha: str) -> Optional[Dict]:
        """
        Get the diff for a specific commit to extract changed Java files
        
        Args:
            repo_name: Repository full name
            commit_sha: Commit SHA
            
        Returns:
            Commit diff information
        """
        url = f"{self.base_url}/repos/{repo_name}/commits/{commit_sha}"
        data = self.make_api_request(url)
        
        if not data:
            return None
        
        # Filter for Java files only
        java_files = []
        for file_info in data.get('files', []):
            filename = file_info.get('filename', '')
            if filename.endswith('.java') and file_info.get('patch'):
                java_files.append({
                    'filename': filename,
                    'status': file_info.get('status'),
                    'additions': file_info.get('additions', 0),
                    'deletions': file_info.get('deletions', 0),
                    'patch': file_info.get('patch'),
                    'raw_url': file_info.get('raw_url')
                })
        
        if java_files:
            return {
                'sha': commit_sha,
                'message': data['commit']['message'],
                'author': data['commit']['author'],
                'date': data['commit']['author']['date'],
                'repository': repo_name,
                'java_files': java_files,
                'stats': data.get('stats', {}),
                'url': data.get('html_url')
            }
        
        return None
    
    def run_scraping_pipeline(self, output_file: str = 'java_vulnerability_commits.json', 
                            max_repos: int = 100, max_commits_per_repo: int = 50):
        """
        Run the complete scraping pipeline
        
        Args:
            output_file: File to save the results
            max_repos: Maximum repositories to process
            max_commits_per_repo: Maximum commits per repository
        """
        logger.info("Starting Java vulnerability commit scraping pipeline...")
        
        # Step 1: Search for repositories
        logger.info("Step 1: Searching for Java repositories...")
        repositories = self.search_repositories(max_repos)
        
        # Step 2: Search for vulnerability commits in each repository
        logger.info("Step 2: Searching for vulnerability commits...")
        all_commits = []
        
        for i, repo in enumerate(repositories):
            logger.info(f"Processing repository {i+1}/{len(repositories)}: {repo['name']}")
            
            commits = self.search_vulnerability_commits(repo['name'], max_commits_per_repo)
            
            # Step 3: Get detailed diff information for each commit
            for commit in commits:
                diff_info = self.get_commit_diff(repo['name'], commit['sha'])
                if diff_info and diff_info['java_files']:
                    all_commits.append(diff_info)
                    
            # Save intermediate results every 10 repositories
            if (i + 1) % 10 == 0:
                with open(f"intermediate_{output_file}", 'w') as f:
                    json.dump(all_commits, f, indent=2)
                logger.info(f"Saved intermediate results: {len(all_commits)} commits")
        
        # Step 4: Save final results
        logger.info(f"Saving final results to {output_file}...")
        with open(output_file, 'w') as f:
            json.dump(all_commits, f, indent=2)
        
        logger.info(f"Scraping complete! Collected {len(all_commits)} vulnerability-fixing commits")
        return all_commits

# Example usage
if __name__ == "__main__":
    # Load GitHub token from file (create a file named 'github_token.txt' with your token)
    try:
        with open('github_token.txt', 'r') as f:
            token = f.read().strip()
    except FileNotFoundError:
        print("Please create a 'github_token.txt' file with your GitHub personal access token")
        print("Get one from: https://github.com/settings/tokens")
        exit(1)
    
    scraper = JavaVulnerabilityCommitScraper(token)
    
    # Run the scraping pipeline
    commits = scraper.run_scraping_pipeline(
        output_file='java_vulnerability_commits.json',
        max_repos=50,  # Start with smaller number for testing
        max_commits_per_repo=30
    )
    
    print(f"Successfully collected {len(commits)} vulnerability-fixing commits!")