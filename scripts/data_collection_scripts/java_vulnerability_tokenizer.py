#!/usr/bin/env python3
"""
Professional Java Code Tokenizer for Vulnerability Detection
Implements proper Java syntax understanding using JavaParser equivalent
"""

import javalang
import json
import logging
import re
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
import hashlib

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JavaVulnerabilityTokenizer:
    """
    Professional Java tokenizer that understands syntax, semantics, and vulnerability patterns
    """
    
    def __init__(self):
        self.java_keywords = {
            'abstract', 'assert', 'boolean', 'break', 'byte', 'case', 'catch', 'char',
            'class', 'const', 'continue', 'default', 'do', 'double', 'else', 'enum',
            'extends', 'final', 'finally', 'float', 'for', 'goto', 'if', 'implements',
            'import', 'instanceof', 'int', 'interface', 'long', 'native', 'new',
            'package', 'private', 'protected', 'public', 'return', 'short', 'static',
            'strictfp', 'super', 'switch', 'synchronized', 'this', 'throw', 'throws',
            'transient', 'try', 'void', 'volatile', 'while'
        }
        
        # Security-sensitive APIs and patterns
        self.security_apis = {
            'sql_injection': [
                'PreparedStatement', 'Statement', 'createStatement', 'prepareStatement',
                'executeQuery', 'executeUpdate', 'execute', 'setString', 'setInt'
            ],
            'xss': [
                'HttpServletRequest', 'HttpServletResponse', 'getParameter', 'getWriter',
                'print', 'println', 'setAttribute', 'sendRedirect'
            ],
            'deserialization': [
                'ObjectInputStream', 'readObject', 'readUnshared', 'ObjectOutputStream',
                'writeObject', 'Serializable', 'Externalizable'
            ],
            'file_operations': [
                'File', 'FileInputStream', 'FileOutputStream', 'Path', 'Paths',
                'Files', 'createFile', 'write', 'read'
            ],
            'crypto': [
                'MessageDigest', 'Cipher', 'SecretKey', 'KeyGenerator', 'SecureRandom',
                'getInstance', 'doFinal', 'update'
            ]
        }
        
        self.annotation_patterns = {
            '@RequestMapping', '@GetMapping', '@PostMapping', '@PathVariable',
            '@RequestParam', '@Autowired', '@Service', '@Controller', '@RestController'
        }
    
    def preprocess_java_code(self, code: str) -> str:
        """Clean and normalize Java code for tokenization"""
        # Remove single-line comments but preserve structure
        code = re.sub(r'//[^\n]*', '', code)
        
        # Handle multi-line comments
        code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
        
        # Normalize whitespace but preserve structure
        code = re.sub(r'\s+', ' ', code)
        
        # Preserve string literals with placeholder
        string_literals = []
        def replace_string(match):
            string_literals.append(match.group(0))
            return f'STRING_LITERAL_{len(string_literals)-1}'
        
        code = re.sub(r'"[^"]*"', replace_string, code)
        code = re.sub(r"'[^']*'", replace_string, code)
        
        return code.strip()
    
    def tokenize_java_code(self, code: str, preserve_structure: bool = True) -> List[Dict]:
        """
        Tokenize Java code with syntax awareness and vulnerability context
        """
        try:
            # Preprocess code
            cleaned_code = self.preprocess_java_code(code)
            
            # Parse with javalang
            tokens = []
            try:
                token_generator = javalang.tokenizer.tokenize(cleaned_code)
                
                for token in token_generator:
                    token_info = {
                        'value': token.value,
                        'type': token.__class__.__name__,
                        'position': getattr(token, 'position', None),
                        'security_context': self._analyze_security_context(token.value),
                        'semantic_type': self._classify_semantic_type(token.value),
                        'normalized_value': self._normalize_token(token.value)
                    }
                    tokens.append(token_info)
                    
            except javalang.tokenizer.LexerError as e:
                logger.warning(f"Lexer error: {e}, falling back to simple tokenization")
                tokens = self._fallback_tokenization(cleaned_code)
            
            return tokens
            
        except Exception as e:
            logger.error(f"Tokenization failed: {e}")
            return self._fallback_tokenization(code)
    
    def _analyze_security_context(self, token_value: str) -> Dict:
        """Analyze if token is security-relevant"""
        context = {
            'is_security_api': False,
            'vulnerability_types': [],
            'risk_level': 'low'
        }
        
        for vuln_type, apis in self.security_apis.items():
            if token_value in apis:
                context['is_security_api'] = True
                context['vulnerability_types'].append(vuln_type)
                context['risk_level'] = 'high'
        
        return context
    
    def _classify_semantic_type(self, token_value: str) -> str:
        """Classify token into semantic categories"""
        if token_value.lower() in self.java_keywords:
            return 'keyword'
        elif token_value in self.annotation_patterns:
            return 'annotation'
        elif token_value.startswith('STRING_LITERAL'):
            return 'string_literal'
        elif token_value.isdigit():
            return 'numeric_literal'
        elif re.match(r'^[A-Z][a-zA-Z0-9]*$', token_value):
            return 'class_name'
        elif re.match(r'^[a-z][a-zA-Z0-9]*$', token_value):
            return 'identifier'
        elif token_value in '(){}[];,.':
            return 'separator'
        elif token_value in '+-*/=<>!&|':
            return 'operator'
        else:
            return 'other'
    
    def _normalize_token(self, token_value: str) -> str:
        """Normalize token for better embedding learning"""
        semantic_type = self._classify_semantic_type(token_value)
        
        if semantic_type == 'string_literal':
            return 'STRING_LITERAL'
        elif semantic_type == 'numeric_literal':
            return 'NUMBER_LITERAL'
        elif semantic_type == 'class_name':
            return 'CLASS_NAME'
        elif semantic_type in ['keyword', 'operator', 'separator']:
            return token_value.lower()
        else:
            return token_value
    
    def _fallback_tokenization(self, code: str) -> List[Dict]:
        """Simple fallback tokenization when JavaLang fails"""
        simple_tokens = re.findall(r'\w+|[^\w\s]', code)
        
        tokens = []
        for token in simple_tokens:
            token_info = {
                'value': token,
                'type': 'Unknown',
                'position': None,
                'security_context': self._analyze_security_context(token),
                'semantic_type': self._classify_semantic_type(token),
                'normalized_value': self._normalize_token(token)
            }
            tokens.append(token_info)
        
        return tokens
    
    def extract_vulnerability_context(self, tokens: List[Dict], window_size: int = 10) -> List[Dict]:
        """
        Extract vulnerability-relevant contexts from tokenized code
        """
        vulnerability_contexts = []
        
        for i, token in enumerate(tokens):
            if token['security_context']['is_security_api']:
                # Extract context window around security-sensitive token
                start_idx = max(0, i - window_size)
                end_idx = min(len(tokens), i + window_size + 1)
                
                context = {
                    'center_token': token,
                    'context_tokens': tokens[start_idx:end_idx],
                    'context_window': [t['normalized_value'] for t in tokens[start_idx:end_idx]],
                    'vulnerability_types': token['security_context']['vulnerability_types'],
                    'position_in_context': i - start_idx
                }
                
                vulnerability_contexts.append(context)
        
        return vulnerability_contexts
    
    def process_commit_data(self, commit_data: Dict) -> Dict:
        """
        Process commit data with professional Java tokenization
        """
        processed_commit = {
            'commit_sha': commit_data.get('sha'),
            'repository': commit_data.get('repository'),
            'message': commit_data.get('message'),
            'java_files': [],
            'vulnerability_contexts': [],
            'token_statistics': defaultdict(int)
        }
        
        for java_file in commit_data.get('java_files', []):
            filename = java_file.get('filename', 'unknown')
            patch = java_file.get('patch', '')
            
            if not patch:
                continue
            
            # Parse patch to extract before/after code
            before_code, after_code = self._extract_code_from_patch(patch)
            
            if before_code and after_code:
                # Tokenize both versions
                before_tokens = self.tokenize_java_code(before_code)
                after_tokens = self.tokenize_java_code(after_code)
                
                # Extract vulnerability contexts
                before_contexts = self.extract_vulnerability_context(before_tokens)
                after_contexts = self.extract_vulnerability_context(after_tokens)
                
                processed_file = {
                    'filename': filename,
                    'before_tokens': before_tokens,
                    'after_tokens': after_tokens,
                    'before_contexts': before_contexts,
                    'after_contexts': after_contexts,
                    'token_diff': self._compute_token_diff(before_tokens, after_tokens)
                }
                
                processed_commit['java_files'].append(processed_file)
                processed_commit['vulnerability_contexts'].extend(before_contexts)
                
                # Update statistics
                for token in before_tokens:
                    processed_commit['token_statistics'][token['normalized_value']] += 1
        
        return processed_commit
    
    def _extract_code_from_patch(self, patch: str) -> Tuple[str, str]:
        """Extract before and after code from git patch"""
        lines = patch.split('\n')
        before_lines = []
        after_lines = []
        
        for line in lines:
            if line.startswith('-') and not line.startswith('---'):
                before_lines.append(line[1:])
            elif line.startswith('+') and not line.startswith('+++'):
                after_lines.append(line[1:])
            elif not line.startswith(('@@', 'diff', 'index', '---', '+++')):
                # Context line - appears in both
                before_lines.append(line)
                after_lines.append(line)
        
        return '\n'.join(before_lines), '\n'.join(after_lines)
    
    def _compute_token_diff(self, before_tokens: List[Dict], after_tokens: List[Dict]) -> Dict:
        """Compute meaningful differences between token sequences"""
        before_values = [t['normalized_value'] for t in before_tokens]
        after_values = [t['normalized_value'] for t in after_tokens]
        
        return {
            'removed_tokens': list(set(before_values) - set(after_values)),
            'added_tokens': list(set(after_values) - set(before_values)),
            'token_change_ratio': len(set(before_values).symmetric_difference(set(after_values))) / max(len(before_values), 1)
        }


def main():
    """Test the professional Java tokenizer"""
    tokenizer = JavaVulnerabilityTokenizer()
    
    # Test with sample Java code
    sample_code = '''
    public class VulnerableExample {
        public void sqlInjection(String userInput) {
            String sql = "SELECT * FROM users WHERE name = '" + userInput + "'";
            Statement stmt = connection.createStatement();
            ResultSet rs = stmt.executeQuery(sql);
        }
        
        @RequestMapping("/vulnerable")
        public void xssVulnerable(HttpServletRequest request, HttpServletResponse response) {
            String input = request.getParameter("data");
            response.getWriter().print("Hello " + input);
        }
    }
    '''
    
    tokens = tokenizer.tokenize_java_code(sample_code)
    contexts = tokenizer.extract_vulnerability_context(tokens)
    
    print(f"Tokenized {len(tokens)} tokens")
    print(f"Found {len(contexts)} vulnerability contexts")
    
    for context in contexts:
        print(f"Vulnerability types: {context['vulnerability_types']}")
        print(f"Context: {' '.join(context['context_window'])}")
        print("---")

if __name__ == "__main__":
    main()