#!/usr/bin/env python3
"""
Vulnerability Type Balancer
Balance dataset across vulnerability types following Wartschinski's methodology
"""

import json
import hashlib
import logging
import numpy as np
from typing import List, Dict, Tuple
from collections import Counter, defaultdict
import random
from java_dataset_processor import JavaDatasetProcessor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VulnerabilityTypeBalancer:
    def __init__(self, context_window: int = 10):
        self.context_window = context_window
        self.processor = JavaDatasetProcessor(context_window)
        
        # Target distribution for balanced dataset
        self.target_distribution = {
            'sql_injection': 0.20,      # 20%
            'xss': 0.18,                # 18%
            'command_injection': 0.15,  # 15%
            'deserialization': 0.12,    # 12%
            'path_traversal': 0.12,     # 12%
            'xxe': 0.10,                # 10%
            'csrf': 0.08,               # 8%
            'unknown': 0.05             # 5% (minimize unknown)
        }
        
        # Sampling strategies per vulnerability type
        self.sampling_strategies = {
            'oversample': ['xss', 'xxe', 'deserialization', 'csrf'],  # Increase these
            'undersample': ['unknown', 'path_traversal'],              # Reduce these
            'maintain': ['sql_injection', 'command_injection']         # Keep current levels
        }
        
        # Quality thresholds for different vulnerability types
        self.quality_thresholds = {
            'sql_injection': 0.7,      # High confidence required
            'xss': 0.6,                # Medium-high confidence
            'command_injection': 0.7,   # High confidence required
            'deserialization': 0.6,     # Medium-high confidence
            'path_traversal': 0.5,      # Medium confidence (common)
            'xxe': 0.8,                # Very high confidence (rare)
            'csrf': 0.6,               # Medium-high confidence
            'unknown': 0.9             # Only keep very high quality unknowns
        }
    
    def analyze_current_distribution(self, commits_data: List[Dict]) -> Dict:
        """Analyze current vulnerability type distribution"""
        logger.info("Analyzing current vulnerability type distribution...")
        
        # Process commits to get sample counts
        all_samples = []
        commit_vuln_types = []
        
        for commit in commits_data:
            # Get vulnerability type from commit
            vuln_type = (commit.get('validated_vulnerability_type') or 
                        commit.get('target_vulnerability') or 
                        'unknown')
            commit_vuln_types.append(vuln_type)
            
            # Process commit to estimate sample count
            java_files = commit.get('java_files', [])
            estimated_samples = 0
            
            for java_file in java_files:
                patch = java_file.get('patch', '')
                if patch:
                    # Estimate samples from patch size
                    lines = len(patch.split('\n'))
                    estimated_samples += max(1, lines // 5)  # Rough estimate
            
            all_samples.append({
                'commit_sha': commit.get('sha'),
                'vulnerability_type': vuln_type,
                'estimated_samples': estimated_samples,
                'quality_score': commit.get('quality_scores', {}).get('overall', 0.5)
            })
        
        # Calculate current distribution
        current_counts = Counter(commit_vuln_types)
        total_commits = len(commits_data)
        
        current_distribution = {
            vuln_type: count / total_commits 
            for vuln_type, count in current_counts.items()
        }
        
        # Estimate sample distribution
        sample_counts = defaultdict(int)
        for sample in all_samples:
            sample_counts[sample['vulnerability_type']] += sample['estimated_samples']
        
        total_samples = sum(sample_counts.values())
        sample_distribution = {
            vuln_type: count / total_samples 
            for vuln_type, count in sample_counts.items()
        }
        
        analysis = {
            'commit_counts': dict(current_counts),
            'commit_distribution': current_distribution,
            'estimated_sample_counts': dict(sample_counts),
            'estimated_sample_distribution': sample_distribution,
            'total_commits': total_commits,
            'total_estimated_samples': total_samples,
            'distribution_gaps': {}
        }
        
        # Calculate gaps from target distribution
        for vuln_type, target_pct in self.target_distribution.items():
            current_pct = sample_distribution.get(vuln_type, 0.0)
            gap = target_pct - current_pct
            analysis['distribution_gaps'][vuln_type] = {
                'target': target_pct,
                'current': current_pct,
                'gap': gap,
                'gap_samples': int(gap * total_samples),
                'action_needed': 'increase' if gap > 0.02 else 'decrease' if gap < -0.02 else 'maintain'
            }
        
        return analysis
    
    def prioritize_commits_by_quality(self, commits_data: List[Dict], 
                                    vuln_type: str, action: str) -> List[Dict]:
        """Prioritize commits based on quality scores and vulnerability type"""
        
        # Filter commits by vulnerability type
        vuln_commits = []
        for commit in commits_data:
            commit_vuln_type = (commit.get('validated_vulnerability_type') or 
                              commit.get('target_vulnerability') or 
                              'unknown')
            
            if commit_vuln_type == vuln_type:
                quality_score = commit.get('quality_scores', {}).get('overall', 0.5)
                threshold = self.quality_thresholds.get(vuln_type, 0.6)
                
                # Apply quality threshold
                if quality_score >= threshold:
                    vuln_commits.append(commit)
        
        # Sort by quality score (descending for increase, ascending for decrease)
        if action == 'increase':
            vuln_commits.sort(key=lambda x: x.get('quality_scores', {}).get('overall', 0.5), 
                            reverse=True)
        else:
            vuln_commits.sort(key=lambda x: x.get('quality_scores', {}).get('overall', 0.5))
        
        return vuln_commits
    
    def augment_underrepresented_types(self, commits_data: List[Dict], 
                                     analysis: Dict) -> List[Dict]:
        """Augment underrepresented vulnerability types through intelligent sampling"""
        logger.info("Augmenting underrepresented vulnerability types...")
        
        augmented_commits = list(commits_data)  # Start with original data
        augmentation_stats = {}
        
        for vuln_type, gap_info in analysis['distribution_gaps'].items():
            if gap_info['action_needed'] == 'increase' and gap_info['gap'] > 0.05:
                logger.info(f"Augmenting {vuln_type} (gap: {gap_info['gap']:.2%})")
                
                # Get high-quality commits for this vulnerability type
                quality_commits = self.prioritize_commits_by_quality(
                    commits_data, vuln_type, 'increase'
                )
                
                if not quality_commits:
                    logger.warning(f"No high-quality commits found for {vuln_type}")
                    continue
                
                # Calculate how many additional samples we need
                target_additional = max(1, gap_info['gap_samples'] // 2)  # Conservative approach
                
                # Intelligent duplication with variation
                augmented_count = 0
                max_augmentations_per_commit = 3
                
                for commit in quality_commits:
                    if augmented_count >= target_additional:
                        break
                    
                    # Create variations of high-quality commits
                    for variation in range(min(max_augmentations_per_commit, 
                                             target_additional - augmented_count)):
                        
                        # Create augmented commit with slight modifications
                        augmented_commit = self.create_commit_variation(
                            commit, variation_type=f'augment_{variation}'
                        )
                        
                        augmented_commits.append(augmented_commit)
                        augmented_count += 1
                
                augmentation_stats[vuln_type] = {
                    'original_commits': len(quality_commits),
                    'augmented_commits': augmented_count,
                    'target_gap': gap_info['gap_samples']
                }
                
                logger.info(f"Augmented {vuln_type}: +{augmented_count} commits")
        
        logger.info(f"Augmentation complete: {len(augmented_commits) - len(commits_data)} commits added")
        return augmented_commits, augmentation_stats
    
    def create_commit_variation(self, original_commit: Dict, variation_type: str) -> Dict:
        """Create a variation of a commit for augmentation"""
        
        # Create a deep copy of the original commit
        import copy
        varied_commit = copy.deepcopy(original_commit)
        
        # Modify identifiers to make it unique
        original_sha = varied_commit.get('sha', 'unknown')
        varied_commit['sha'] = f"{original_sha}_{variation_type}"
        varied_commit['augmentation_source'] = original_sha
        varied_commit['augmentation_type'] = variation_type
        varied_commit['is_augmented'] = True
        
        # Slightly modify quality scores to add variation
        if 'quality_scores' in varied_commit:
            original_score = varied_commit['quality_scores']['overall']
            # Add small random variation (±5%)
            variation = random.uniform(-0.05, 0.05)
            new_score = max(0.0, min(1.0, original_score + variation))
            varied_commit['quality_scores']['overall'] = new_score
        
        return varied_commit
    
    def downsample_overrepresented_types(self, commits_data: List[Dict], 
                                       analysis: Dict) -> List[Dict]:
        """Downsample overrepresented vulnerability types"""
        logger.info("Downsampling overrepresented vulnerability types...")
        
        filtered_commits = []
        downsampling_stats = {}
        
        # Group commits by vulnerability type
        commits_by_type = defaultdict(list)
        for commit in commits_data:
            vuln_type = (commit.get('validated_vulnerability_type') or 
                        commit.get('target_vulnerability') or 
                        'unknown')
            commits_by_type[vuln_type].append(commit)
        
        for vuln_type, commits in commits_by_type.items():
            gap_info = analysis['distribution_gaps'].get(vuln_type, {})
            action_needed = gap_info.get('action_needed', 'maintain')
            
            if action_needed == 'decrease' and gap_info.get('gap', 0) < -0.05:
                logger.info(f"Downsampling {vuln_type} (excess: {-gap_info['gap']:.2%})")
                
                # Sort by quality and keep only the best ones
                quality_sorted = self.prioritize_commits_by_quality(
                    commits, vuln_type, 'decrease'
                )
                
                # Calculate how many to keep
                target_reduction = abs(gap_info['gap_samples'])
                commits_to_keep = max(1, len(commits) - (target_reduction // 15))  # ~15 samples per commit
                
                kept_commits = quality_sorted[:commits_to_keep]
                filtered_commits.extend(kept_commits)
                
                downsampling_stats[vuln_type] = {
                    'original_commits': len(commits),
                    'kept_commits': len(kept_commits),
                    'removed_commits': len(commits) - len(kept_commits)
                }
                
                logger.info(f"Downsampled {vuln_type}: {len(commits)} → {len(kept_commits)} commits")
            else:
                # Keep all commits for this type
                filtered_commits.extend(commits)
        
        logger.info(f"Downsampling complete: {len(commits_data) - len(filtered_commits)} commits removed")
        return filtered_commits, downsampling_stats
    
    def balance_vulnerability_types(self, input_files: List[str], 
                                  output_file: str = 'balanced_vulnerability_dataset.json') -> Dict:
        """Main function to balance vulnerability types across input files"""
        logger.info("Starting vulnerability type balancing...")
        
        # Load and combine all input data
        all_commits = []
        for input_file in input_files:
            try:
                with open(input_file, 'r') as f:
                    commits = json.load(f)
                    if isinstance(commits, list):
                        all_commits.extend(commits)
                    logger.info(f"Loaded {len(commits)} commits from {input_file}")
            except Exception as e:
                logger.error(f"Error loading {input_file}: {e}")
                continue
        
        logger.info(f"Total commits loaded: {len(all_commits)}")
        
        # Analyze current distribution
        analysis = self.analyze_current_distribution(all_commits)
        
        # Print current distribution
        logger.info("Current vulnerability type distribution:")
        for vuln_type, info in analysis['distribution_gaps'].items():
            logger.info(f"  {vuln_type}: {info['current']:.1%} (target: {info['target']:.1%}, "
                       f"gap: {info['gap']:+.1%})")
        
        # Step 1: Downsample overrepresented types
        downsampled_commits, downsample_stats = self.downsample_overrepresented_types(
            all_commits, analysis
        )
        
        # Step 2: Augment underrepresented types
        balanced_commits, augment_stats = self.augment_underrepresented_types(
            downsampled_commits, analysis
        )
        
        # Step 3: Process commits into training samples
        logger.info("Processing balanced commits into training samples...")
        samples = self.processor.process_commits_dataset(
            commits_file=None,  # We'll pass data directly
            output_file=output_file,
            max_samples_per_commit=40,
            commits_data=balanced_commits  # Pass data directly
        )
        
        # Final analysis
        final_analysis = self.analyze_final_distribution(samples)
        
        # Create comprehensive balancing report
        balancing_report = {
            'balancing_timestamp': logging.Formatter().formatTime(logging.LogRecord(
                '', '', '', 0, '', (), '')),
            'input_summary': {
                'input_files': input_files,
                'total_input_commits': len(all_commits),
                'final_commits': len(balanced_commits),
                'final_samples': len(samples)
            },
            'target_distribution': self.target_distribution,
            'original_analysis': analysis,
            'downsampling_stats': downsample_stats,
            'augmentation_stats': augment_stats,
            'final_distribution': final_analysis,
            'balancing_effectiveness': self.calculate_balancing_effectiveness(
                analysis, final_analysis
            )
        }
        
        # Save balancing report
        report_file = output_file.replace('.json', '_balancing_report.json')
        with open(report_file, 'w') as f:
            json.dump(balancing_report, f, indent=2)
        
        # Generate summary
        self.print_balancing_summary(balancing_report)
        
        return balancing_report
    
    def analyze_final_distribution(self, samples: List[Dict]) -> Dict:
        """Analyze final distribution after balancing"""
        vuln_type_counts = Counter()
        
        for sample in samples:
            vuln_type = sample.vulnerability_type
            vuln_type_counts[vuln_type] += 1
        
        total_samples = len(samples)
        final_distribution = {
            vuln_type: count / total_samples 
            for vuln_type, count in vuln_type_counts.items()
        }
        
        return {
            'sample_counts': dict(vuln_type_counts),
            'sample_distribution': final_distribution,
            'total_samples': total_samples
        }
    
    def calculate_balancing_effectiveness(self, original_analysis: Dict, 
                                        final_analysis: Dict) -> Dict:
        """Calculate how effective the balancing was"""
        effectiveness = {}
        
        original_dist = original_analysis['estimated_sample_distribution']
        final_dist = final_analysis['sample_distribution']
        
        for vuln_type, target_pct in self.target_distribution.items():
            original_gap = abs(target_pct - original_dist.get(vuln_type, 0.0))
            final_gap = abs(target_pct - final_dist.get(vuln_type, 0.0))
            
            improvement = original_gap - final_gap
            improvement_pct = (improvement / original_gap) * 100 if original_gap > 0 else 0
            
            effectiveness[vuln_type] = {
                'original_gap': original_gap,
                'final_gap': final_gap,
                'improvement': improvement,
                'improvement_percentage': improvement_pct
            }
        
        # Overall effectiveness
        avg_improvement = sum(e['improvement_percentage'] for e in effectiveness.values()) / len(effectiveness)
        effectiveness['overall'] = {
            'average_improvement_percentage': avg_improvement,
            'types_improved': len([e for e in effectiveness.values() if e['improvement'] > 0]),
            'types_degraded': len([e for e in effectiveness.values() if e['improvement'] < 0])
        }
        
        return effectiveness
    
    def print_balancing_summary(self, report: Dict):
        """Print comprehensive balancing summary"""
        print("\n" + "="*80)
        print("⚖️ VULNERABILITY TYPE BALANCING RESULTS")
        print("="*80)
        
        input_summary = report['input_summary']
        print(f"📊 Input Summary:")
        print(f"   Input commits: {input_summary['total_input_commits']}")
        print(f"   Final commits: {input_summary['final_commits']}")
        print(f"   Final samples: {input_summary['final_samples']}")
        
        print(f"\n🎯 Target vs Final Distribution:")
        final_dist = report['final_distribution']['sample_distribution']
        for vuln_type, target_pct in self.target_distribution.items():
            final_pct = final_dist.get(vuln_type, 0.0)
            diff = final_pct - target_pct
            status = "✅" if abs(diff) < 0.03 else "⚠️" if abs(diff) < 0.05 else "❌"
            print(f"   {vuln_type}: {final_pct:.1%} (target: {target_pct:.1%}) {status}")
        
        print(f"\n📈 Balancing Effectiveness:")
        effectiveness = report['balancing_effectiveness']
        overall = effectiveness['overall']
        print(f"   Average improvement: {overall['average_improvement_percentage']:.1f}%")
        print(f"   Types improved: {overall['types_improved']}")
        print(f"   Types degraded: {overall['types_degraded']}")
        
        print(f"\n🔧 Actions Taken:")
        if 'downsampling_stats' in report and report['downsampling_stats']:
            print(f"   Downsampling:")
            for vuln_type, stats in report['downsampling_stats'].items():
                print(f"     {vuln_type}: {stats['original_commits']} → {stats['kept_commits']} commits")
        
        if 'augmentation_stats' in report and report['augmentation_stats']:
            print(f"   Augmentation:")
            for vuln_type, stats in report['augmentation_stats'].items():
                print(f"     {vuln_type}: +{stats['augmented_commits']} commits")
        
        print("="*80)

    def process_commits_dataset_direct(self, commits_data: List[Dict], 
                                     output_file: str, 
                                     max_samples_per_commit: int = 40) -> List:
        """Process commits data directly without file I/O"""
        logger.info(f"Processing {len(commits_data)} commits into training samples...")
        
        all_samples = []
        vulnerability_type_counts = {}
        
        for i, commit_data in enumerate(commits_data):
            if i % 50 == 0:
                logger.info(f"Processing commit {i+1}/{len(commits_data)}")
            
            commit_samples = []
            
            # Process each Java file in the commit
            for java_file in commit_data.get('java_files', []):
                if 'patch' not in java_file:
                    continue
                
                # Parse the diff
                hunks = self.processor.parse_diff_hunks(java_file['patch'])
                
                if not hunks:
                    continue
                
                # Create labeled samples
                file_samples = self.processor.create_labeled_samples(
                    hunks=hunks,
                    filename=java_file['filename'],
                    commit_sha=commit_data['sha'],
                    commit_message=commit_data['message'],
                    repository=commit_data['repository']
                )
                
                commit_samples.extend(file_samples)
            
            # Limit samples per commit
            if len(commit_samples) > max_samples_per_commit:
                commit_samples = commit_samples[:max_samples_per_commit]
            
            all_samples.extend(commit_samples)
            
            # Track vulnerability types
            for sample in commit_samples:
                vuln_type = sample.vulnerability_type
                vulnerability_type_counts[vuln_type] = vulnerability_type_counts.get(vuln_type, 0) + 1
        
        logger.info(f"Created {len(all_samples)} training samples")
        
        # Convert to serializable format
        dataset = []
        for sample in all_samples:
            dataset.append({
                'tokens': sample.tokens,
                'labels': sample.labels,
                'context_window': sample.context_window,
                'filename': sample.filename,
                'commit_sha': sample.commit_sha,
                'vulnerability_type': sample.vulnerability_type,
                'repository': sample.repository,
                'sample_id': hashlib.md5(
                    (sample.commit_sha + sample.filename + str(sample.tokens)).encode()
                ).hexdigest()[:8]
            })
        
        # Save the dataset
        logger.info(f"Saving dataset to {output_file}...")
        with open(output_file, 'w') as f:
            json.dump(dataset, f, indent=2)
        
        # Save statistics
        stats = {
            'total_samples': len(dataset),
            'total_commits': len(commits_data),
            'vulnerability_type_distribution': vulnerability_type_counts,
            'average_tokens_per_sample': sum(len(s['tokens']) for s in dataset) / len(dataset) if dataset else 0,
            'context_window': self.context_window
        }
        
        with open(f"{output_file.replace('.json', '_stats.json')}", 'w') as f:
            json.dump(stats, f, indent=2)
        
        return dataset

# Extended JavaDatasetProcessor to accept direct data
class ExtendedJavaDatasetProcessor(JavaDatasetProcessor):
    def process_commits_dataset(self, commits_file: str = None, 
                               output_file: str = 'java_vulnerability_dataset.json',
                               max_samples_per_commit: int = 50,
                               commits_data: List[Dict] = None):
        """Extended version that can work with direct data or file"""
        
        if commits_data is not None:
            # Work with provided data directly
            commits_data_to_process = commits_data
        else:
            # Load from file as usual
            logger.info("Loading commits data...")
            with open(commits_file, 'r') as f:
                commits_data_to_process = json.load(f)
        
        logger.info(f"Processing {len(commits_data_to_process)} commits...")
        
        all_samples = []
        vulnerability_type_counts = {}
        
        for i, commit_data in enumerate(commits_data_to_process):
            logger.info(f"Processing commit {i+1}/{len(commits_data_to_process)}: {commit_data['sha'][:8]}")
            
            commit_samples = []
            
            # Process each Java file in the commit
            for java_file in commit_data.get('java_files', []):
                if 'patch' not in java_file:
                    continue
                
                # Parse the diff
                hunks = self.parse_diff_hunks(java_file['patch'])
                
                if not hunks:
                    continue
                
                # Create labeled samples
                file_samples = self.create_labeled_samples(
                    hunks=hunks,
                    filename=java_file['filename'],
                    commit_sha=commit_data['sha'],
                    commit_message=commit_data['message'],
                    repository=commit_data['repository']
                )
                
                commit_samples.extend(file_samples)
            
            # Limit samples per commit to avoid overrepresentation
            if len(commit_samples) > max_samples_per_commit:
                commit_samples = commit_samples[:max_samples_per_commit]
            
            all_samples.extend(commit_samples)
            
            # Track vulnerability types
            for sample in commit_samples:
                vuln_type = sample.vulnerability_type
                vulnerability_type_counts[vuln_type] = vulnerability_type_counts.get(vuln_type, 0) + 1
        
        logger.info(f"Created {len(all_samples)} training samples")
        logger.info("Vulnerability type distribution:")
        for vuln_type, count in sorted(vulnerability_type_counts.items()):
            logger.info(f"  {vuln_type}: {count} samples")
        
        # Convert to serializable format
        dataset = []
        for sample in all_samples:
            dataset.append({
                'tokens': sample.tokens,
                'labels': sample.labels,
                'context_window': sample.context_window,
                'filename': sample.filename,
                'commit_sha': sample.commit_sha,
                'vulnerability_type': sample.vulnerability_type,
                'repository': sample.repository,
                'sample_id': hashlib.md5(
                    (sample.commit_sha + sample.filename + str(sample.tokens)).encode()
                ).hexdigest()[:8]
            })
        
        # Save the dataset
        logger.info(f"Saving dataset to {output_file}...")
        with open(output_file, 'w') as f:
            json.dump(dataset, f, indent=2)
        
        # Save statistics
        stats = {
            'total_samples': len(dataset),
            'total_commits': len(commits_data_to_process),
            'vulnerability_type_distribution': vulnerability_type_counts,
            'average_tokens_per_sample': sum(len(s['tokens']) for s in dataset) / len(dataset) if dataset else 0,
            'context_window': self.context_window
        }
        
        with open(f"{output_file.replace('.json', '_stats.json')}", 'w') as f:
            json.dump(stats, f, indent=2)
        
        logger.info("Dataset processing complete!")
        return all_samples

def main():
    """Main execution function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Balance Java Vulnerability Dataset")
    parser.add_argument('input_files', nargs='+', help='Input JSON files with commits')
    parser.add_argument('--output', '-o', default='balanced_vulnerability_dataset.json',
                      help='Output file for balanced dataset')
    parser.add_argument('--context-window', type=int, default=10,
                      help='Context window size for token samples')
    
    args = parser.parse_args()
    
    # Initialize balancer with extended processor
    balancer = VulnerabilityTypeBalancer(args.context_window)
    balancer.processor = ExtendedJavaDatasetProcessor(args.context_window)
    
    # Run balancing
    report = balancer.balance_vulnerability_types(args.input_files, args.output)
    
    print(f"\n✅ Vulnerability type balancing complete!")
    print(f"📊 Balanced dataset: {args.output}")
    print(f"📋 Balancing report: {args.output.replace('.json', '_balancing_report.json')}")

if __name__ == "__main__":
    main()